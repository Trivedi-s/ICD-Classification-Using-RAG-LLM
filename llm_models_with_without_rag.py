# -*- coding: utf-8 -*-
"""LLM_Models_With_Without_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ba9FvccwI85zVpsbmK-KFTikR_91Kwp

**Environment Setup & Dependencies Installation**
"""

!pip install -q datasets fsspec transformers accelerate scikit-learn faiss-cpu evaluate rouge_score huggingface_hub

import os, numpy as np, pandas as pd, torch, faiss, nltk
from datasets import load_dataset
from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, classification_report
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,
    Seq2SeqTrainer, Seq2SeqTrainingArguments
)
from huggingface_hub import HfApi

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

nltk.download('punkt', quiet=True)

"""**RAG Mode Configuration (Enable/Disable Retrieval Augmentation)**"""

USE_RAG_TRAIN = True
USE_RAG_EVAL  = True
TEST_FRACTION = 0.10

"""**Load Datasets**"""

mimic_train_full = load_dataset("rntc/mimic-icd-visit", split="train", download_mode="force_redownload")
mimic_test_full  = load_dataset("rntc/mimic-icd-visit", split="test", download_mode="force_redownload")
icd10_data       = load_dataset("awacke1/ICD10-Clinical-Terminology", split="train", download_mode="force_redownload")

print(mimic_train_full.column_names)
print(icd10_data.column_names)

"""**ICD-10 Knowledge Base & Retrieval System Setup**"""

from transformers import AutoTokenizer as HFTokenizer, AutoModel as HFModel

icd10_kb_texts = [f"{row['Code'].strip().upper()} - {row['Description'].strip()}"
            for row in icd10_data]

retriever_model_name = "emilyalsentzer/Bio_ClinicalBERT"
retriever_tokenizer = HFTokenizer.from_pretrained(retriever_model_name)
retriever_model = HFModel.from_pretrained(retriever_model_name).to(DEVICE).eval()


def compute_mean_pool(last_hidden_state, attention_mask):
    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    masked_output = last_hidden_state * mask
    summed_mask = masked_output.sum(dim=1)
    token_counts = torch.clamp(mask.sum(dim=1), min=1e-9)
    return summed_mask / token_counts

@torch.no_grad()
def encode_texts(texts, max_length=256, batch_size=64):
    if len(texts) == 0:
        return np.zeros((0, 768), dtype=np.float32)

    embeddings = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        tokens = retriever_tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        ).to(DEVICE)

        hidden_states = retriever_model(**tokens).last_hidden_state
        pooled_output = compute_mean_pool(hidden_states, tokens["attention_mask"])
        normalized = torch.nn.functional.normalize(pooled_output, p=2, dim=1)  # cosine-ready
        embeddings.append(normalized.cpu().numpy())

    return np.vstack(embeddings)

try:
    index = faiss.read_index("icd10_kb.index")
    print("Loaded FAISS index from disk.")
except Exception:
    print("Encoding ICD-10 KBâ€¦")
    kb_embs = encode_texts(icd10_kb_texts, max_length=256, batch_size=64)
    index = faiss.IndexFlatIP(kb_embs.shape[1])
    index.add(kb_embs)
    faiss.write_index(index, "icd10_kb.index")
    print("KB vectors indexed:", index.ntotal)

@torch.no_grad()
def retrieve_icd_snippets(query_text, top_k=5):
    query_embedding = encode_texts([query_text], max_length=256, batch_size=1)
    if query_embedding.shape[0] == 0:
        return []

    distances, indices = index.search(query_embedding, top_k)
    return [icd10_kb_texts[i] for i in indices[0]]

"""**Data Preprocessing**"""

def preprocess_mimic_row(example, use_rag=True, top_k=5):
    cleaned_text = (example.get("cleaned_text") or "").replace("\n", " ").replace("\t", " ").strip().lower()

    if use_rag:
        try:
            retrieved_snippets = retrieve_icd_snippets(cleaned_text, top_k=top_k)
            rag_context = " ".join([f"[ICD] {snippet}" for snippet in retrieved_snippets])
            input_text = f"{cleaned_text}\n\nicd context: {rag_context}"
        except Exception:
            input_text = cleaned_text
    else:
        input_text = cleaned_text

    codes = example.get("icd_code", [])
    target_text = ",".join(sorted(set(str(code).strip().upper() for code in codes)))

    return {"input_text": input_text, "target_text": target_text}

def preprocess_mimic_batch(batch, use_rag=True, top_k=5):
    input_texts = []
    target_texts = []

    for text, codes in zip(batch.get("cleaned_text", []), batch.get("icd_code", [])):
        cleaned_text = (text or "").replace("\n", " ").replace("\t", " ").strip().lower()

        if use_rag:
            try:
                retrieved_snippets = retrieve_icd_snippets(cleaned_text, top_k=top_k)
                rag_context = " ".join([f"[ICD] {snippet}" for snippet in retrieved_snippets])
                input_text = f"{cleaned_text}\n\nicd context: {rag_context}"
            except Exception:
                input_text = cleaned_text
        else:
            input_text = cleaned_text

        target_text = ",".join(sorted(set(str(code).strip().upper() for code in codes)))

        input_texts.append(input_text)
        target_texts.append(target_text)

    return {"input_text": input_texts, "target_text": target_texts}

split_dataset = mimic_train_full.train_test_split(test_size=0.2, seed=42)

train_dataset = split_dataset["train"]
val_dataset = split_dataset["test"]

"""**Training and Deploy Model to Hugging Face Hub**"""

!pip install -U huggingface_hub
from huggingface_hub import login

login()

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)
from huggingface_hub import HfApi
import torch

def run_training_with_rag(
    model_name: str,
    hf_repo_id: str,
    run_name: str,
    train_dataset,
    val_dataset,
    use_rag: bool = False,
    is_checkpoint: bool = False,
    output_dir_base: str = "./checkpoints"
):
    print(f"\n Training {model_name} | RAG: {use_rag}\n{'-'*60}")

    # Create repo on Hugging Face
    HfApi().create_repo(repo_id=hf_repo_id, private=False, exist_ok=True)

    # Preprocess using custom RAG logic
    train_dataset = train_dataset.map(lambda b: preprocess_mimic_batch(b, use_rag=use_rag), batched=True)
    val_dataset   = val_dataset.map(lambda b: preprocess_mimic_batch(b, use_rag=use_rag), batched=True)

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

    def tokenize_function(example):
        model_inputs = tokenizer(
            example["input_text"],
            max_length=512,
            padding="max_length",
            truncation=True
        )
        labels = tokenizer(
            text_target=example["target_text"],
            max_length=64,
            padding="max_length",
            truncation=True
        )
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)
    tokenized_val   = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)

    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

    output_dir = f"{output_dir_base}/{model_name.replace('/', '')}_{'rag' if use_rag else 'plain'}"

    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        logging_steps=100,
        eval_strategy="epoch",
        save_strategy="steps",
        save_steps=500,
        save_total_limit=2,
        predict_with_generate=True,
        fp16=torch.cuda.is_available(),
        report_to="wandb",
        run_name=run_name
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    trainer.train(resume_from_checkpoint=is_checkpoint)

    # Push to Hugging Face Hub
    model.save_pretrained(hf_repo_id, push_to_hub=True)
    tokenizer.save_pretrained(hf_repo_id, push_to_hub=True)
    print(f"Uploaded to: https://huggingface.co/{hf_repo_id}")

# T5-small without RAG
run_training_with_rag(
    model_name="t5-small",
    hf_repo_id="ShlokaTrivedi/mimic-icd-t5-small-plain",
    run_name="t5_small_plain",
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    use_rag=False,
    is_checkpoint=False
)

# T5-small with RAG
run_training_with_rag(
    model_name="t5-small",
    hf_repo_id="ShlokaTrivedi/mimic-icd-t5-small-rag",
    run_name="t5_small_rag",
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    use_rag=True,
    is_checkpoint=False
)

# BART-base without RAG
run_training_with_rag(
    model_name="facebook/bart-base",
    hf_repo_id="ShlokaTrivedi/mimic-icd-bart-plain",
    run_name="bart_plain",
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    use_rag=False,
    is_checkpoint=False
)

# BART-base with RAG
run_training_with_rag(
    model_name="facebook/bart-base",
    hf_repo_id="ShlokaTrivedi/mimic-icd-bart-rag",
    run_name="bart_rag",
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    use_rag=True,
    is_checkpoint=False
)

"""**Prepare test set with retrieval-augmented context for ICD code prediction**"""

from datasets import load_dataset

test_data = load_dataset("rntc/mimic-icd-visit", split="test").shuffle(seed=42).select(range(500))

def preprocess_mimic(batch, use_rag=False, top_k=5):
    input_texts, target_texts = [], []

    for text, codes in zip(batch["cleaned_text"], batch["icd_code"]):
        cleaned = text.replace("\n", " ").replace("\t", " ").strip().lower()

        if use_rag:
            try:
                retrieved_snippets = retrieve_icd_snippets(cleaned, top_k=top_k)
                rag_context = " ".join([f"[ICD] {s}" for s in retrieved_snippets])
                cleaned += f"\n\nicd context: {rag_context}"
            except:
                pass

        label = ",".join(sorted(set(code.strip().upper() for code in codes)))
        input_texts.append(cleaned)
        target_texts.append(label)

    return {"input_text": input_texts, "target_text": target_texts}

"""**Comparative Model Evaluation: RAG vs Non-RAG Performance**"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

model_repo_map = {
    ("t5-small", False): "ShlokaTrivedi/mimic-icd-t5-small-plain",
    ("t5-small", True):  "ShlokaTrivedi/mimic-icd-t5-small-rag",
    ("facebook/bart-base", False): "ShlokaTrivedi/mimic-icd-bart-plain",
    ("facebook/bart-base", True):  "ShlokaTrivedi/mimic-icd-bart-rag"
}

results_table = []

for (model_name, use_rag), repo_id in model_repo_map.items():
    print(f"\nEvaluating: {repo_id}")
    model_tag = f"{model_name.replace('/', '-')}-{'rag' if use_rag else 'plain'}"

    processed_test = test_data.map(lambda b: preprocess_mimic(b, use_rag=use_rag), batched=True)

    tokenizer = AutoTokenizer.from_pretrained(repo_id)
    model = AutoModelForSeq2SeqLM.from_pretrained(repo_id).to(DEVICE)

    def tokenize(example):
        model_inputs = tokenizer(
            example["input_text"],
            max_length=512,
            padding="max_length",
            truncation=True
        )
        labels = tokenizer(
            text_target=example["target_text"],
            max_length=64,
            padding="max_length",
            truncation=True
        )
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized = processed_test.map(tokenize, batched=True)

    trainer = Seq2SeqTrainer(
        model=model,
        tokenizer=tokenizer,
        args=Seq2SeqTrainingArguments(
            output_dir="./eval_tmp",
            per_device_eval_batch_size=8,
            predict_with_generate=True,
            do_train=False,
            do_eval=False,
            report_to="none"
        ),
        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
    )

    results = trainer.predict(tokenized)
    decoded_preds = tokenizer.batch_decode(results.predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(results.label_ids, skip_special_tokens=True)

    def to_label_set(label_list):
        return [set(item.split(',')) for item in label_list]

    true_labels = to_label_set(decoded_labels)
    pred_labels = to_label_set(decoded_preds)

    all_labels = sorted(set().union(*true_labels, *pred_labels))
    label2id = {label: i for i, label in enumerate(all_labels)}

    def binarize(label_sets):
        bin = []
        for label_set in label_sets:
            row = [0] * len(label2id)
            for label in label_set:
                if label in label2id:
                    row[label2id[label]] = 1
            bin.append(row)
        return bin

    y_true = binarize(true_labels)
    y_pred = binarize(pred_labels)

    # Metrics
    h_loss = hamming_loss(y_true, y_pred)
    f1_micro = f1_score(y_true, y_pred, average='micro')
    f1_macro = f1_score(y_true, y_pred, average='macro')
    precision = precision_score(y_true, y_pred, average='micro')
    recall = recall_score(y_true, y_pred, average='micro')

    print("Hamming Loss:", h_loss)
    print("Micro F1 Score:", f1_micro)
    print("Macro F1 Score:", f1_macro)
    print("Precision:", precision)
    print("Recall:", recall)

    results_table.append({
        "model": model_tag,
        "hamming_loss": h_loss,
        "f1_micro": f1_micro,
        "f1_macro": f1_macro,
        "precision": precision,
        "recall": recall
    })

    pd.DataFrame({
        "input_text": processed_test["input_text"],
        "true_icd_codes": processed_test["target_text"],
        "predicted_icd_codes": decoded_preds
    }).to_csv(f"predictions_{model_tag}.csv", index=False)

    # Save final table
    results_df = pd.DataFrame(results_table)
    results_df.to_csv("combined_model_metrics.csv", index=False)
    print("\n Combined metrics saved to: combined_model_metrics.csv")

    print("\n Final Evaluation Table:")
    print(results_df)